{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use HDF5 files in Python \n",
    "[source](https://pythonforthelab.com/blog/how-to-use-hdf5-files-in-python/)\n",
    "\n",
    "Datasets in hdf5\n",
    "- Their info is stored on the hard drive and no not load to RAM if unused\n",
    "\n",
    "Chunk size\n",
    "- It is recommended to keep the total size of your chunks between 10 KiB and 1 MiB, larger for larger datasets\n",
    "- when any element in a chunk is accessed, the entire chunk is read from disk\n",
    "- auto chunking is set by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.6174143729211554\n",
      "0.0\n",
      "[-0.83943907 -1.0197887  -1.57003876  0.98782958 -0.71319319]\n",
      "My Data Set\n",
      "My second Data Set\n",
      "0.0\n",
      "[0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "arr = np.random.randn(1000)\n",
    "arr2 = np.random.randn(1000)*0\n",
    "\n",
    "\"\"\"Write to file\"\"\"\n",
    "# open file with w: write permission\n",
    "# with removes the need to use .close()\n",
    "with h5py.File('random.hdf5', 'w') as f:\n",
    "    # create dataset with the following name and data \n",
    "    dset = f.create_dataset(\"My Data Set\", data=arr)\n",
    "    dset2 = f.create_dataset(\"My second Data Set\", data=arr2)\n",
    "\n",
    "\n",
    "\"\"\" Read file \"\"\"\n",
    "# using r: read attribute\n",
    "with h5py.File('random.hdf5', 'r') as f:\n",
    "   # data is pointing to the file and not loaded to memory \n",
    "   # you can see this in the Jupyter variables \n",
    "   data = f['My Data Set']\n",
    "   data2 = f['My second Data Set']\n",
    "   print(min(data))\n",
    "   print(max(data2))\n",
    "   print(data[:5])\n",
    "\n",
    "   # keep a set of values \n",
    "   data_set = data2[:5]\n",
    "   \n",
    "   # behavior of the data set is similar to a dictionary\n",
    "   for key in f.keys():\n",
    "        print(key)\n",
    "\n",
    "print(data_set[4])\n",
    "# print(data2[1]) # the dataset is locked but the variable we created is not\n",
    "\n",
    "\"\"\" Placing the Data into RAM \"\"\"\n",
    "# done by including [()]\n",
    "f = h5py.File('random.hdf5', 'r')\n",
    "data_in_RAM = f['My second Data Set'][()]\n",
    "f.close()\n",
    "print(data_in_RAM[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex Data sets\n",
    "Strategy: If a data set is too large we can loop through the disk rather than attempt to read it all to RAM at once "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of data with a for loop: 5089\n",
      "0.3575088283136155\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Loading via Loop\"\"\"\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "arr1 = np.random.randn(10000)\n",
    "arr2 = np.random.randn(10000)\n",
    "\n",
    "with h5py.File('complex_read.hdf5', 'w') as f:\n",
    "    f.create_dataset('array_1', data=arr1)\n",
    "    f.create_dataset('array_2', data=arr2)\n",
    "\n",
    "\n",
    "with h5py.File('complex_read.hdf5', 'r') as f:\n",
    "    d1 = f['array_1']\n",
    "    d2 = f['array_2']\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for i in range(len(d1)):\n",
    "        if d1[i] > 0:\n",
    "            data.append(d2[i])\n",
    "\n",
    "print('The length of data with a for loop: {}'.format(len(data)))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Loading all at once \"\"\"\n",
    "\n",
    "with h5py.File('complex_read.hdf5', 'r') as f:\n",
    "    d1 = f['array_1']\n",
    "    d2 = f['array_2']\n",
    "\n",
    "    data = d2[d1[()]>0] # all of d1 is loaded and some of d2 is taken\n",
    "\n",
    "print(data[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153.0\n",
      "[[  0.   0.   0. ...   0.   0.   0.]\n",
      " [  0. 123. 123. ... 123. 123. 123.]\n",
      " [  0. 123. 123. ... 123. 123. 123.]\n",
      " ...\n",
      " [  0. 123. 123. ... 123. 123. 123.]\n",
      " [  0. 123. 123. ... 123. 123. 123.]\n",
      " [  0. 123. 123. ... 123. 123. 123.]]\n"
     ]
    }
   ],
   "source": [
    "arr = np.random.randn(100)\n",
    "\n",
    "with h5py.File('random.hdf5', 'w') as f:\n",
    "   # here we have a 1d dataset set to 1000 entries\n",
    "   dset = f.create_dataset(\"default\", (1000,))\n",
    "   dset_matrix = f.create_dataset('matrix', (500, 1024))\n",
    "\n",
    "   # you must specify where you are storing \n",
    "   # something like dset = arr will NOT work\n",
    "   dset[10:20] = arr[50:60]\n",
    "\n",
    "   #setting some values \n",
    "   dset_matrix[1,2] = 153\n",
    "   dset_matrix[200:500, 500:1024] = 123\n",
    "\n",
    "   this = dset_matrix[()]\n",
    "\n",
    "print(this[1,2])\n",
    "print(this[199:500, 499:1024])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types\n",
    "- `i1`: int 1 byte\n",
    "- `i8`: int 8 bytes \n",
    "- `c16`: complex numbers of 16 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n",
      "1200\n",
      "(4+3j)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('several_datasets.hdf5', 'w') as f:\n",
    "   dset_int_1 = f.create_dataset('integers', (10, ), dtype='i1')\n",
    "   dset_int_8 = f.create_dataset('integers8', (10, ), dtype='i8')\n",
    "   dset_complex = f.create_dataset('complex', (10, ), dtype='c16')\n",
    "\n",
    "   dset_int_1[0] = 127\n",
    "   dset_int_8[0] = 1200.1\n",
    "   dset_complex[0] = 3j + 4\n",
    "\n",
    "   print(dset_int_1[0])\n",
    "   print(dset_int_8[0])\n",
    "   print(dset_complex[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compression \n",
    "- default level is 4 \n",
    "- high in example is 9 \n",
    "- 0 is no compression \n",
    "\n",
    "> Ints compress better than floats \n",
    "\n",
    "Float compression: \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{no compression} &= 1602144 \\\\\n",
    "    \\text{compression 4} &= 1469868 \\\\\n",
    "    \\text{compression 9} &= 1469580 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "about 8% compression gained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "arr = np.random.randn(100000)\n",
    "level = 0\n",
    "\n",
    "with h5py.File('integer_1_compr.hdf5', 'w') as f:\n",
    "    d = f.create_dataset('dataset', (100000,), dtype='i1', compression=\"gzip\", compression_opts= level)\n",
    "    d[:] = arr\n",
    "\n",
    "with h5py.File('integer_8_compr.hdf5', 'w') as f:\n",
    "    d = f.create_dataset('dataset', (100000,), dtype='i8', compression=\"gzip\", compression_opts=level)\n",
    "    d[:] = arr\n",
    "\n",
    "with h5py.File('float_compr.hdf5', 'w') as f:\n",
    "    d = f.create_dataset('dataset', (100000,), dtype='f16', compression=\"gzip\", compression_opts=level)\n",
    "    d[:] = arr\n",
    "\n",
    "# Use the terminal to see the file byte sizes by level \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
